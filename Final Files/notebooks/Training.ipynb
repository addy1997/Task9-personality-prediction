{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKt2Pgz0hwLu"
      },
      "source": [
        "#import packages\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten, Reshape\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras.constraints import unitnorm\n",
        "from keras.regularizers import l2\n",
        "from keras.initializers import random_uniform\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-sW8tFphzZB"
      },
      "source": [
        "def get_idx_from_sent(sent, word_idx_map, max_l=51, kernel_size=5):\n",
        "    \"\"\"\n",
        "    Transforms sentence into a list of indices. Pad with zeroes.\n",
        "    \"\"\"\n",
        "    x = []\n",
        "    pad = kernel_size - 1\n",
        "    for i in range(pad):\n",
        "        x.append(0)\n",
        "    words = sent.split()\n",
        "    for word in words:\n",
        "        if word in word_idx_map:\n",
        "            x.append(word_idx_map[word])\n",
        "    while len(x) < max_l+2*pad:\n",
        "        x.append(0)\n",
        "    return x\n",
        "\n",
        "def make_idx_data(revs, word_idx_map, max_l=51, kernel_size=5):\n",
        "    \"\"\"\n",
        "    Transforms sentences into a 2-d matrix.\n",
        "    \"\"\"\n",
        "    train, val, test = [], [], []\n",
        "    for rev in revs:\n",
        "        sent = get_idx_from_sent(rev['text'], word_idx_map, max_l, kernel_size)\n",
        "        sent.append(rev['y'])\n",
        "        if rev['split'] == 1:\n",
        "            train.append(sent)\n",
        "        elif rev['split'] == 0:\n",
        "            val.append(sent)\n",
        "    train = np.array(train, dtype=np.int)\n",
        "    val = np.array(val, dtype=np.int)\n",
        "    return [train, val]\n",
        "\n",
        "\n",
        "print (\"loading data...\")\n",
        "\n",
        "with open(\"__trait__.pickle\", 'rb') as f:\n",
        "    x = pickle.load(f, encoding='latin')\n",
        "revs, W, word_idx_map, vocab = x[0], x[1], x[2], x[3]\n",
        "print (\"data loaded!\")\n",
        "\n",
        "datasets = make_idx_data(revs, word_idx_map, max_l=2721,kernel_size=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpKLSkKfR6lA"
      },
      "source": [
        "# Train data preparation\n",
        "N = datasets[0].shape[0]\n",
        "conv_input_width = W.shape[1]\n",
        "conv_input_height = int(datasets[0].shape[1]-1)\n",
        "\n",
        "# For each word write a word index (not vector) to X tensor\n",
        "train_X = np.zeros((N, conv_input_height), dtype=np.int)\n",
        "train_Y = np.zeros((N, 2), dtype=np.int)\n",
        "for i in range(N):\n",
        "    for j in range(conv_input_height):\n",
        "        train_X[i, j] = datasets[0][i, j]\n",
        "    \n",
        "print ('train_X.shape = {}'.format(train_X.shape))\n",
        "print ('train_Y.shape = {}'.format(train_Y.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkLESZ7Z9mi2"
      },
      "source": [
        "import pandas as pd\n",
        "url = 'https://raw.githubusercontent.com/addy1997/Task9-personality-prediction/main/essays.csv'\n",
        "data_train = pd.read_csv(url, encoding='latin')\n",
        "for i in range(N):\n",
        "    train_Y[i,data_train.iloc[i,3]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avmdnj9Y9rzX"
      },
      "source": [
        "print(train_X.shape)\n",
        "print(train_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLFl5AW6ZX_E"
      },
      "source": [
        "# Validation data preparation\n",
        "Nv = datasets[1].shape[0]\n",
        "\n",
        "# For each word write a word index (not vector) to X tensor\n",
        "val_X = np.zeros((Nv, conv_input_height), dtype=np.int)\n",
        "val_Y = np.zeros((Nv, 2), dtype=np.int)\n",
        "for i in range(Nv):\n",
        "    for j in range(conv_input_height):\n",
        "        val_X[i, j] = datasets[1][i, j]\n",
        "    \n",
        "print ('val_X.shape = {}'.format(val_X.shape))\n",
        "print ('val_Y.shape = {}'.format(val_Y.shape))\n",
        "for i in range(Nv):\n",
        "    val_Y[i,data_train.iloc[i,3]] = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-fGOcjnTAYK"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Atin17/Personality_Prediction_using_Twitter/master/essays.csv'\n",
        "data_train = pd.read_csv(url, encoding='latin')\n",
        "for i in range(N):\n",
        "    train_Y[i,data_train.iloc[i,3]] = 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2cDTz-gTDeZ"
      },
      "source": [
        "print(train_X.shape)\n",
        "print(train_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVmacoUbZjsr"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n",
        "from tensorflow.keras.constraints import UnitNorm\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.initializers import random_uniform\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "backend.set_image_data_format('channels_first')\n",
        "\n",
        "# Number of feature maps (outputs of convolutional layer)\n",
        "N_fm = 300\n",
        "# kernel size of convolutional layer\n",
        "kernel_size = 8\n",
        "\n",
        "model = Sequential()\n",
        "# Embedding layer (lookup table of trainable word vectors)\n",
        "model.add(Embedding(input_dim=W.shape[0], \n",
        "                    output_dim=W.shape[1], \n",
        "                    input_length=conv_input_height,\n",
        "                    weights=[W]))\n",
        "# Reshape word vectors from Embedding to tensor format suitable for Convolutional layer\n",
        "model.add(Reshape((1, conv_input_height, conv_input_width)))\n",
        "\n",
        "# first convolutional layer\n",
        "model.add(Convolution2D(N_fm, \n",
        "                        kernel_size, \n",
        "                        conv_input_width, \n",
        "                        padding='same', \n",
        "                        kernel_regularizer=l2(0.0001)))\n",
        "# ReLU activation\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# aggregate data in every feature map to scalar using MAX operation\n",
        "model.add(MaxPooling2D(pool_size=(conv_input_height-kernel_size+1, 1), padding='same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "# Inner Product layer (as in regular neural network, but without non-linear activation function)\n",
        "model.add(Dense(2))\n",
        "# SoftMax activation; actually, Dense+SoftMax works as Multinomial Logistic Regression\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Custom optimizers could be used, though right now standard adadelta is employed\n",
        "opt = Adadelta(lr=1.0, rho=0.95, epsilon=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', \n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwF1uSC79zMW"
      },
      "source": [
        "model.weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwsCz2lsmOdl"
      },
      "source": [
        "model.fit(x=train_X,y=train_Y,batch_size=32,epochs=15,verbose=1, validation_data=(val_X,val_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m89Hjrme6Bk"
      },
      "source": [
        "cvscores=[]\n",
        "scores = model.evaluate(val_X, val_Y, verbose=1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "cvscores.append(scores[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_cTljYbevcP"
      },
      "source": [
        "model.save('__trait__.model')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}